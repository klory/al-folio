---
layout: post
title: T-test
description: 
date: 2022-07-29
comments: true
---

# Use case

Determine if our guess/prediction/hypothesis is significant.

## Examples

One-sample t-test:

* [[Toss coin](https://www.probabilitycourse.com/chapter8/8_4_4_p_vals.php) expl] If we toss a coin 100 times with 60 heads, can we trust it to be a fair coin?

* [[Pearson](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html) expl] Given two sequence x (feature) and y (label), we can compute [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) r. Assume r is high (e.g., 0.8), how do we trust this result, what if both sequences are actually randomly sampled and this is just a coincidence? In other words, how to know if this result is **significantly** different compared with "assuming x and y are randomly sampled"? How **confident** are we about the result?


Two-independent-sample t-test:

* [[Two independent sample mean](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html) expl] We are given two sequence a and b which may be sampled from the same one-dimensional random variable. E.g., we get 13 samples in a~[1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26] and 11 samples in b~[2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21], can we trust their means are actually the same?

# [Knowledge] Hypothesis testing

From [here](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/): Hypothesis testing in statistics is a way for you to test the results of a survey or experiment to see if you have meaningful results. You’re basically testing whether your results are valid by figuring out the odds that your results have happened by chance. If your results may have happened by chance, the experiment won’t be repeatable and so has little use.

# [Knowledge] Null Hypothesis
From [here](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/) again: The null hypothesis is always the accepted fact.

## State a Null Hypothesis
This is a little tricky, you make a hypothesis based on experiment result, stating a null hypothesis means "assuming your result still follows the accepted fact", let's try [some exercises](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/):

> [Patient recovery expl] Researcher thinks that if knee surgery patients go to physical therapy twice a week (instead of 3 times), their recovery period will be longer. Average recovery times for knee surgery patients is 8.2 weeks, experiemnt result on 40 patients shows the average is 8.3 weeks.

Here, the accepted fact is $$\mu=8.2$$, researcher hypothesizes "$$\mu>8.2$$ when therapy twick a week". So the null hypothesis is "$$\mu=8.2$$ when therapy twice a week". In other words, twice a week has no effect, the patients still only need 8.2 weeks to recover. And we are computing under the null hypothesis, how likely (probability) the result (8.3 weeks over 40 patients) will happen?

> [IQ expl] A principal at a certain school claims that the students in his school are above average intelligence. A random sample of thirty students IQ scores have a mean score of 112.5. Is there sufficient evidence to support the principal’s claim? The mean population IQ is 100 with a standard deviation of 15.

Here, the accepted fact is "IQ=100", the principle hypothesizes "IQ>100 in his school", so the null hypothesis is "IQ=100 in his school". And we are computing under the null hypothesis, how likely (probability) the result (IQ=112.5 over 30 students) will happen?

> [Blood glucose expl] Blood glucose levels for obese patients have a mean of 100 with a standard deviation of 15. A researcher thinks that a diet high in raw cornstarch will have **a positive or negative** effect on blood glucose levels. A sample of 30 patients who have tried the raw cornstarch diet have a mean glucose level of 102.

Here, the accepted fact is "$$\mu=100$$", the researcher hypothesizes "$$\mu<100$$ or $$\mu>100$$ with cornstarch", so the null hypothesis is "$$\mu=100$$ with cornstarch". And we are computing under the null hypothesis, how likely (probability) the result ($$\mu>=102$$ or $$\mu<=98$$ over 30 patients, [why?](#oneside_vs_twoside)) will happen?

Now let's try more:

> The above [Toss coin expl]

Here, the accepted fact is "coin is fair", the computed result hypothesizes "This coin is not fair with P(head)=0.6", so the null hypothesis is "This coin is fair". And we are computing under the null hypothesis, how likely (probability) the result ($$P(head)=0.6$$) will happen?

> The above [Pearson expl]

Here, the accepted fact is "if data are randomly sampled, they should have no correlation, in other words, the pearson correlation coeeficient should be $$r_0=0$$", the computed result hypothesizes "This {x,y} have correlation $$r_{x,y}=x$$", so the null hypothesis is "This {x,y} have no correlation". And we are computing under the null hypothesis, how likely (probability) the result ($$r_{x,y}==x$$) will happen?


> The above [Two independent sample means expl]

Here, the accepted fact is "if two sequence are sampled from same random variable, they should have identical mean", the computed result hypothesizes "These two sequences {a,b} are not from the same random variable, and their mean differenc is $$\Delta$$", so the null hypothesis is "These two sequences {a,b} are sampled from same random variable". And we are computing under the null hypothesis, how likely (probability) the result (mean difference is $$\Delta$$) will happen?

Now let's try more and more:

> [Classification expl] You develop a binary classification model with accuracy 98%, can you trust this accuracy?

Here, the accepted fact is "if we randomly guess the result, we will have an accuracy 50%", the reuslt hypothesizes "This is not random guess (the model works)", so the null hypothesis is "This is still random guess". And we are computing under the null hypothesis, how likely (probability) the result (accuracy is 90%) will happen?

> [Regression expl] You develop a regression model which has mse (mean squared error) 0.2, can you trust this mse?

Here, the accepted fact is "if we randomly guess the result, we will have an mse $$err_0$$", the reuslt hypothesizes "This is not random guess (the model works)", so the null hypothesis is "This is still random guess". And we are computing under the null hypothesis, how likely (probability) the result (mse is 0.2) will happen?

## Conclusion of Null Hypothesis
We now understand what is null hypothesis and how to state it. And at the end of each example, I also throw out a question, the question is actually what t-test (or z-test) tries to answer, more generally, **under null hypothesis, how like the result will happen?**

# [Knowledge] Z-test

Before we go into t-test, let's first look at a simpler case, which is z-test, in this case, we know both the population mean and std ([IQ expl] and [Blood glucose expl]).

> The above [IQ expl]

Population mean $$\mu_0=100$$, population std $$\sigma_0=15$$, experiment result mean $$\mu = 112.5$$, experiment sample size $$n=30$$.

According to z-score equation, we know the z-score is

$$Z = \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}$$

$$ Z = \frac{112.5 - 100}{15 / \sqrt{30}} = 4.56$$

This tells the sample mean is $$4.56\sigma_0$$ away from the population mean $$\mu_0$$, if you look up z score table, you will see the corresponding area under curve is more than 99.99%.

![](/assets/img/2022-07-29-t-test/gaussian_distribution.png)

Well, how do we interprete this 99.99%? Intuitively, this tells us 

* Under the null hypothesis, such a result is very unlikely to happen (so far away from the mean). Or,
* Our sample mean is very unlikely to see natually (under the null hypothesis). Or,
* The result **rejects the null hypothesis**. Or,
* The principal is very likely to be correct, student here indeed have higher IQ (assume the princpal is not cherry-picking clever students from his/her school)

Here, we introduce another concept "reject null hypothesis", and this is actually all we want to do, we do experiments, design models to find something new, something **significantly** different from "accepted fact", don't we?

But statistics/math do not like "very likely" or "significantly", we want to define it more accurately, so now we will interprete the result from another perspective to more accurately express our result as "we are xx% (confidence level) sure about the result is correct".

# [Knowledge] Alpha (Significance level), P-value, and Confidence Interval
(I don't know why people invented so many concepts, while actually they are all saying the same thing)

## Alpha
Assume in the [IQ expl], the confidence level (area under curve (AUC)) we got is 99.99%, then alpha would be:

$$ \alpha = 1 - AUC = 0.0001 $$

But this is not how we use alpha in real world, in real world, we often 

1. Pre-select an alpha value (say 0.05) before the experiment, then 
2. Find the z-score corresponding to the alpha value (I know, believe me I don't like this part either), in this case we find the $$z_{\alpha=0.05} = 1.645$$. 
3. Do the experiment, and get our $$z=4.56$$
4. $$z > z_{\alpha=0.05}$$, it means **the null hypothesis is very unlikely to happen**, so we reject null hypothesis
5. We report we are at least 95% sure the students in this school are more clever.

## P-value

> Words in this section are almost the same as in the alpha section.

Assume in the [IQ expl], the confidence level (area under curve (AUC)) we got is 99.99%, then p-value would be:

$$ p = 1 - AUC = 0.0001 $$

So what is the different between alpha and p-value? Well, they are the same thing, it is just alpha is the pre-selected target, and p-value is the actual result.

In real-world, we often:

1. Pre-select an alpha value (say $$z_{\alpha}$$ 0.05) before the experiment, then 
2. ~~Find the z-score corresponding to the alpha value (I know, believe me I don't this part either), in this case we find the $$z_{\alpha=0.05} = 1.645$$.~~
3. Do the experiment, and get our $$z=4.56$$
4. Check the z-score vs. p-value table, find the p-value
5. $$p < z_{\alpha}$$, it means **the null hypothesis is very unlikely to happen**, so we reject null hypothesis
6. We report we are at least 95% sure the students in this school are more clever.


## Confidence Interval (CI)

To compute CI, we need population mean, population std, and sample size, and a pre-selected alpha (or confidence level),

In the [IQ expl], assume we pick alpha=0.05, which correpondes to z=1.96 then the confidence interval is:

$$ left = \mu_0 + (\sigma_0/\sqrt(n)) * 1.96 = 100 + (15/\sqrt{30}) * 1.96 = 105.37 $$

$$ right = +\infty $$

Which tells us if we want to have at least 95% certainty with current sample size, then our sample mean should lies with $$(105.37, \infty)$$, our result 112.5 is in the range, so we will have at least 95% certainty, we reject the null hypothesis.

# One more example before we continue
> [Blood glucose expl]

$$\mu_0=100$$, $$\sigma_0=15$$, $$n=30$$, $$\mu=140$$.

$$ z = \frac{102 - 100}{15 / \sqrt{30}} = 0.73 $$

Looking up in z-score vs. p-value, $$p=0.46539$$, so if we choose $$\alpha=0.05$$, $$p \gt \alpha$$, **the null hypothesis is likely to happen** (very likely in this case), in other words, the result is not significant.

## Larger sample size is always better
But if our sample size is 200, then z=1.88, and p=0.06, which is very close to the target 0.05, intuitively, we are more certain about our result (the sample mean) when we have more samples. This suggests us when we do experiment, we need at least a certain sample size to make sure our result is trustworthy, and the more, the better.

## [One-side vs Two-side](#oneside_vs_twoside)

Note when you look up in the z-score vs. p-value table, it will let you choose "one-side" or "two-side", what is it?

Actually, in [Blood glucose expl], if you put z=0.73 in the standard Gaussian distribution cdf

$$cdf(z)=\frac{1}{2}[1+erf(\frac{z}{\sqrt{2}})]$$

you will get `cdf(0.73) = 0.7673`, which means `p= 1-0.7673 = 0.2327`, but this is only one-side p-value, it means

> If we randomly choose 30 persons (no matter they eat cornstarch or not), we have 23.27% to get a mean blood glucose to be $$\mu_{sample}>=102$$.

However, this is not what we want in the [Blood glucose expl], "a diet high in raw cornstarch will have **a positive or negative effect** on blood glucose levels." This is telling us we need the two-side p-value, which means

> When we see a result (sample mean) = 102, we should interprete it as the blood glucose level should be between 98 and 102, $$98<=\mu_{sample}<=102$$. Here 98 is computed from

$$\mu_0 - (102-\mu_0) = 100 - (102-100) = 98$$

This means we are looking at a harder problem (more difficult to reject the null hypothesis):

![one-side hypothesis vs two-side hypothesis](/assets/img/2022-07-29-t-test/one-side-vs-two-side.jpeg)

The red area is the probability (p-value) that the result (sample mean) will happen under null hypothesis. We see the probability (p-value) in the two-side case is doubled.

So the p-valud in the [blood glucose expl] is `2 * one-side = 2 * 0.2327 = 0.4654`, and since we set our alpha (target value) to be 0.05, $$0.4654 >> 0.05$$, we still have a loooong way to go.



# Limitations of z test

* Know population mean
* Know population std

# Moving to t-distribution

* Know population mean
* ~~Know population std~~

Intuitively, we are saying when we can not get population std, we use sample std instead.

$$t = \frac{\bar{X}-\mu}{S/\sqrt{n}}$$

* $$\bar{X}$$: sample mean
* $$\mu$$: population mean
* $$S$$: sample std
* $$n$$: sample size
* $$t$$: t score

But since we replaced the population std with sample std, t score is not a standard normal distribution anymore, it now follows the [student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution#In_frequentist_statistical_inference). The reason for this is beyond the scope of this post, but anyway, student's t distribution is very much like Gaussian distribution, but only need one parameter (degree of freedom) instead of two in Gaussian (mean and std).

![From Wikipedia](/assets/img/2022-07-29-t-test/t-distribution.jpg)

As you can see, student's t distribution is defined by some complicate function called [gamma function](https://en.wikipedia.org/wiki/Gamma_function), which is beyound the scope of this post. But scipy in Python has already prepare [this](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html) for us. Similar to z-test, we care about the cumulative distribution function (cdf) output, which tells us how like the event score will happen.

So in real world, all we need to do is 

1. Know population mean beforehand.
2. Do the experiment, collect sample size, sample mean, and sample std.
3. Compute t score
4. Compute cdf output ([scipy.stats.t.cdf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html) or just look up it in the [t-table](https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf))

The rest is the same as z-test, so I will conclude the post here.